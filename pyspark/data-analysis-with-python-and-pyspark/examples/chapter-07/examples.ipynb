{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/05 16:52:12 WARN Utils: Your hostname, karlos-300E5M-300E5L resolves to a loopback address: 127.0.1.1; using 10.0.0.89 instead (on interface wlp2s0)\n",
      "23/01/05 16:52:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/05 16:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName('Data Analysis with Python and PySpark - Chapter 07 Examples') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chemical elements example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the saved schema from disk if it exists. Otherwhise, save the infered schema to disk.\n",
    "\n",
    "Therefore, after the first dataset read operation, any subsequent read will not need to infer the schema, which requires Spark to read the dataset two times, one for infer the schema, and one to read the data itself, in contrast to when the schema is not infered, which requires one read operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_elements_schema = False\n",
    "elements_schema = T.StructType()\n",
    "\n",
    "try:\n",
    "    with open('./elements_schema.json', mode='r') as f:\n",
    "        elements_schema = T.StructType.fromJson(json.load(f))\n",
    "except FileNotFoundError:\n",
    "    infer_elements_schema = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AtomicNumber: int, Element: string, Symbol: string, AtomicMass: double, NumberofNeutrons: int, NumberofProtons: int, NumberofElectrons: int, Period: int, Group: int, Phase: string, Radioactive: string, Natural: string, Metal: string, Nonmetal: string, Metalloid: string, Type: string, AtomicRadius: double, Electronegativity: double, FirstIonization: double, Density: double, MeltingPoint: double, BoilingPoint: double, NumberOfIsotopes: int, Discoverer: string, Year: int, SpecificHeat: double, NumberofShells: int, NumberofValence: int]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ELEMENTS_FILEPATH =  '../../data/elements/Periodic_Table_Of_Elements.csv'\n",
    "\n",
    "if infer_elements_schema:\n",
    "    elements = spark.read.csv(\n",
    "        path=ELEMENTS_FILEPATH,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "else:\n",
    "    elements = spark.read.csv(\n",
    "        path=ELEMENTS_FILEPATH,\n",
    "        header=True,\n",
    "        schema=elements_schema,\n",
    "        inferSchema=False\n",
    "    )\n",
    "\n",
    "\n",
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if infer_elements_schema:\n",
    "    with open('./elements_schema.json', mode='w') as f:\n",
    "        json.dump(elements.schema.jsonValue(), f)\n",
    "\n",
    "infer_elements_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+------+----------+----------------+\n",
      "|AtomicNumber|   Element|Symbol|AtomicMass|NumberofNeutrons|\n",
      "+------------+----------+------+----------+----------------+\n",
      "|           1|  Hydrogen|     H|     1.007|               0|\n",
      "|           2|    Helium|    He|     4.002|               2|\n",
      "|           3|   Lithium|    Li|     6.941|               4|\n",
      "|           4| Beryllium|    Be|     9.012|               5|\n",
      "|           5|     Boron|     B|    10.811|               6|\n",
      "|           6|    Carbon|     C|    12.011|               6|\n",
      "|           7|  Nitrogen|     N|    14.007|               7|\n",
      "|           8|    Oxygen|     O|    15.999|               8|\n",
      "|           9|  Fluorine|     F|    18.998|              10|\n",
      "|          10|      Neon|    Ne|     20.18|              10|\n",
      "|          11|    Sodium|    Na|     22.99|              12|\n",
      "|          12| Magnesium|    Mg|    24.305|              12|\n",
      "|          13|  Aluminum|    Al|    26.982|              14|\n",
      "|          14|   Silicon|    Si|    28.086|              14|\n",
      "|          15|Phosphorus|     P|    30.974|              16|\n",
      "|          16|    Sulfur|     S|    32.065|              16|\n",
      "|          17|  Chlorine|    Cl|    35.453|              18|\n",
      "|          18|     Argon|    Ar|    39.948|              22|\n",
      "|          19| Potassium|     K|    39.098|              20|\n",
      "|          20|   Calcium|    Ca|    40.078|              20|\n",
      "+------------+----------+------+----------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.select(elements.columns[:5]).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query the data set to find the number of entries with a liquid state per period."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Period|count|\n",
      "+------+-----+\n",
      "|     4|    1|\n",
      "|     6|    1|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.where(F.col('Phase') == 'liq') \\\n",
    "    .groupby('Period') \\\n",
    "    .count() \\\n",
    "    .orderBy('Period') \\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SQL version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First register a view based on the DataFrame to be able to query it with SQL statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/05 16:52:31 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "+------+--------+\n",
      "|period|count(1)|\n",
      "+------+--------+\n",
      "|     4|       1|\n",
      "|     6|       1|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elements.createOrReplaceTempView('elements')\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT period, COUNT(*)\n",
    "    FROM elements\n",
    "    WHERE phase = \"liq\"\n",
    "    GROUP BY period\n",
    "    ORDER BY period\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage and obtain database metadata with `spark.catalog`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|         | elements|       true|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|current_database()|\n",
      "+------------------+\n",
      "|           default|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT CURRENT_DATABASE()\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set from Backblaze. \n",
    "\n",
    "Reference: https://www.backblaze.com/blog/backblaze-hard-drive-stats-q3-2019/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the saved schema from disk if it exists. Otherwhise, save the infered schema to disk.\n",
    "\n",
    "Therefore, after the first dataset read operation, any subsequent read will not need to infer the schema, which requires Spark to read the dataset two times, one for infer the schema, and one to read the data itself, in contrast to when the schema is not infered, which requires one read operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_backblaze_2019_q3_schema = False\n",
    "backblaze_2019_q3_schema = T.StructType()\n",
    "\n",
    "try:\n",
    "    with open('backblaze_2019_q3', mode='r') as f:\n",
    "        backblaze_2019_q3_schema = T.StructType.fromJson(json.load(f))\n",
    "except FileNotFoundError:\n",
    "    infer_backblaze_2019_q3_schema = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[date: timestamp, serial_number: string, model: string, capacity_bytes: bigint, failure: int, smart_1_normalized: int, smart_1_raw: int, smart_2_normalized: int, smart_2_raw: int, smart_3_normalized: int, smart_3_raw: int, smart_4_normalized: int, smart_4_raw: int, smart_5_normalized: int, smart_5_raw: int, smart_7_normalized: int, smart_7_raw: bigint, smart_8_normalized: int, smart_8_raw: int, smart_9_normalized: int, smart_9_raw: int, smart_10_normalized: int, smart_10_raw: int, smart_11_normalized: int, smart_11_raw: int, smart_12_normalized: int, smart_12_raw: int, smart_13_normalized: string, smart_13_raw: string, smart_15_normalized: string, smart_15_raw: string, smart_16_normalized: int, smart_16_raw: int, smart_17_normalized: int, smart_17_raw: int, smart_22_normalized: int, smart_22_raw: int, smart_23_normalized: int, smart_23_raw: int, smart_24_normalized: int, smart_24_raw: int, smart_168_normalized: int, smart_168_raw: int, smart_170_normalized: int, smart_170_raw: bigint, smart_173_normalized: int, smart_173_raw: bigint, smart_174_normalized: int, smart_174_raw: int, smart_177_normalized: int, smart_177_raw: int, smart_179_normalized: string, smart_179_raw: string, smart_181_normalized: string, smart_181_raw: string, smart_182_normalized: string, smart_182_raw: string, smart_183_normalized: int, smart_183_raw: int, smart_184_normalized: int, smart_184_raw: int, smart_187_normalized: int, smart_187_raw: int, smart_188_normalized: int, smart_188_raw: bigint, smart_189_normalized: int, smart_189_raw: int, smart_190_normalized: int, smart_190_raw: int, smart_191_normalized: int, smart_191_raw: int, smart_192_normalized: int, smart_192_raw: int, smart_193_normalized: int, smart_193_raw: int, smart_194_normalized: int, smart_194_raw: int, smart_195_normalized: int, smart_195_raw: int, smart_196_normalized: int, smart_196_raw: int, smart_197_normalized: int, smart_197_raw: int, smart_198_normalized: int, smart_198_raw: int, smart_199_normalized: int, smart_199_raw: int, smart_200_normalized: int, smart_200_raw: int, smart_201_normalized: string, smart_201_raw: string, smart_218_normalized: int, smart_218_raw: int, smart_220_normalized: int, smart_220_raw: int, smart_222_normalized: int, smart_222_raw: int, smart_223_normalized: int, smart_223_raw: int, smart_224_normalized: int, smart_224_raw: int, smart_225_normalized: int, smart_225_raw: int, smart_226_normalized: int, smart_226_raw: int, smart_231_normalized: int, smart_231_raw: bigint, smart_232_normalized: int, smart_232_raw: bigint, smart_233_normalized: int, smart_233_raw: int, smart_235_normalized: int, smart_235_raw: bigint, smart_240_normalized: int, smart_240_raw: bigint, smart_241_normalized: int, smart_241_raw: bigint, smart_242_normalized: int, smart_242_raw: bigint, smart_250_normalized: int, smart_250_raw: int, smart_251_normalized: int, smart_251_raw: int, smart_252_normalized: int, smart_252_raw: int, smart_254_normalized: int, smart_254_raw: int, smart_255_normalized: string, smart_255_raw: string]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BACKBLAZE_2019_Q3_FILEPATH = '../../data/backblaze/2019_q3/*.csv'\n",
    "\n",
    "if infer_backblaze_2019_q3_schema:\n",
    "    backblaze_2019_q3 = spark.read.csv(\n",
    "        path=BACKBLAZE_2019_Q3_FILEPATH,\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "else:\n",
    "    backblaze_2019_q3 = spark.read.csv(\n",
    "        path=BACKBLAZE_2019_Q3_FILEPATH,\n",
    "        header=True,\n",
    "        schema=backblaze_2019_q3_schema,\n",
    "        inferSchema=False\n",
    "    )\n",
    "\n",
    "backblaze_2019_q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if infer_backblaze_2019_q3_schema:\n",
    "    with open('backblaze_2019_q3.json', mode='w') as f:\n",
    "        json.dump(backblaze_2019_q3.schema.jsonValue(), f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of rows. We have a fairly big dataset, but Spark can handle it well even with only one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:==================================================>     (28 + 3) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10_338_153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f'{backblaze_2019_q3.count():_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+--------------------+--------------+-------+\n",
      "|date               |serial_number |model               |capacity_bytes|failure|\n",
      "+-------------------+--------------+--------------------+--------------+-------+\n",
      "|2019-09-27 00:00:00|Z305B2QN      |ST4000DM000         |4000787030016 |0      |\n",
      "|2019-09-27 00:00:00|ZJV0XJQ4      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZJV0XJQ3      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZJV0XJQ0      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|PL1331LAHG1S4H|HGST HMS5C4040ALE640|4000787030016 |0      |\n",
      "|2019-09-27 00:00:00|ZA16NQJR      |ST8000NM0055        |8001563222016 |0      |\n",
      "|2019-09-27 00:00:00|ZJV02XWG      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZJV1CSVX      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZJV02XWA      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZA18CEBS      |ST8000NM0055        |8001563222016 |0      |\n",
      "|2019-09-27 00:00:00|Z305DEMG      |ST4000DM000         |4000787030016 |0      |\n",
      "|2019-09-27 00:00:00|ZA130TTW      |ST8000DM002         |8001563222016 |0      |\n",
      "|2019-09-27 00:00:00|ZJV5HJQF      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZJV1CSVV      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|ZA18CEBF      |ST8000NM0055        |8001563222016 |0      |\n",
      "|2019-09-27 00:00:00|ZJV02XWV      |ST12000NM0007       |12000138625024|0      |\n",
      "|2019-09-27 00:00:00|PL2331LAG9TEEJ|HGST HMS5C4040ALE640|4000787030016 |0      |\n",
      "|2019-09-27 00:00:00|PL2331LAH3WYAJ|HGST HMS5C4040BLE640|4000787030016 |0      |\n",
      "|2019-09-27 00:00:00|2AGN81UY      |HGST HUH721212ALN604|12000138625024|0      |\n",
      "|2019-09-27 00:00:00|PL1331LAHG53YH|HGST HMS5C4040BLE640|4000787030016 |0      |\n",
      "+-------------------+--------------+--------------------+--------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "backblaze_2019_q3.select('date', 'serial_number', 'model', 'capacity_bytes', 'failure') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='backblaze_2019_q3', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backblaze_2019_q3.createOrReplaceTempView('backblaze_2019_q3')\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick exploratory data analysis on a subset of the columns presented."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check how many different models there are."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_different_models = backblaze_2019_q3.select('model') \\\n",
    "    .distinct() \\\n",
    "    .count()\n",
    "\n",
    "number_of_different_models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 76:======================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|number_of_different_models|\n",
      "+--------------------------+\n",
      "|                        43|\n",
      "+--------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT COUNT(DISTINCT model) AS number_of_different_models\n",
    "    FROM backblaze_2019_q3\n",
    "    \"\"\"\n",
    ").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show a few serial numbers of HDDs which have failed at some point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 87:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|serial_number |\n",
      "+--------------+\n",
      "|ZA10MCJ5      |\n",
      "|ZCH07T9K      |\n",
      "|ZCH0CA7Z      |\n",
      "|Z302F381      |\n",
      "|ZCH0B3Z2      |\n",
      "|PL2331LAGMTS1J|\n",
      "|ZCH0BTJN      |\n",
      "|ZA13QBVZ      |\n",
      "|ZJV00EXD      |\n",
      "|ZCH0ADRN      |\n",
      "|8HJ91VRH      |\n",
      "|ZDEABH54      |\n",
      "|ZCH07C9X      |\n",
      "|ZJV004VF      |\n",
      "|S301M4YT      |\n",
      "|ZCH07VQ8      |\n",
      "|ZJV05KLD      |\n",
      "|ZCH07HHL      |\n",
      "|ZCH073TG      |\n",
      "|ZCH09FCW      |\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "backblaze_2019_q3.select('serial_number') \\\n",
    "    .where('failure = 1') \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|serial_number |\n",
      "+--------------+\n",
      "|ZA10MCJ5      |\n",
      "|ZCH07T9K      |\n",
      "|ZCH0CA7Z      |\n",
      "|Z302F381      |\n",
      "|ZCH0B3Z2      |\n",
      "|PL2331LAGMTS1J|\n",
      "|ZCH0BTJN      |\n",
      "|ZA13QBVZ      |\n",
      "|ZJV00EXD      |\n",
      "|ZCH0ADRN      |\n",
      "|8HJ91VRH      |\n",
      "|ZDEABH54      |\n",
      "|ZCH07C9X      |\n",
      "|ZJV004VF      |\n",
      "|S301M4YT      |\n",
      "|ZCH07VQ8      |\n",
      "|ZJV05KLD      |\n",
      "|ZCH07HHL      |\n",
      "|ZCH073TG      |\n",
      "|ZCH09FCW      |\n",
      "+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT serial_number\n",
    "    FROM backblaze_2019_q3\n",
    "    WHERE failure = 1\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the min and max capacity in GB for each model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 94:======================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------------+-----------------+\n",
      "|model               |min_GB                |max_GB           |\n",
      "+--------------------+----------------------+-----------------+\n",
      "|TOSHIBA MG07ACA14TA |13039.0               |13039.0          |\n",
      "|ST12000NM0117       |11176.0               |11176.0          |\n",
      "|ST12000NM0007       |-9.313225746154785E-10|11176.0          |\n",
      "|HGST HUH721212ALN604|-9.313225746154785E-10|11176.0          |\n",
      "|HGST HUH721212ALE600|11176.0               |11176.0          |\n",
      "|HGST HUH721010ALE600|-9.313225746154785E-10|9314.0           |\n",
      "|ST10000NM0086       |-9.313225746154785E-10|9314.0           |\n",
      "|ST8000DM004         |7452.036460876465     |7452.036460876465|\n",
      "|ST8000DM002         |-9.313225746154785E-10|7452.036460876465|\n",
      "|TOSHIBA HDWF180     |7452.036460876465     |7452.036460876465|\n",
      "|ST8000NM0055        |-9.313225746154785E-10|7452.036460876465|\n",
      "|HGST HUH728080ALE600|-9.313225746154785E-10|7452.036460876465|\n",
      "|ST8000DM005         |7452.036460876465     |7452.036460876465|\n",
      "|WDC WD60EFRX        |5589.02986907959      |5589.02986907959 |\n",
      "|ST6000DM004         |5589.02986907959      |5589.02986907959 |\n",
      "|ST6000DX000         |5589.02986907959      |5589.02986907959 |\n",
      "|ST6000DM001         |5589.02986907959      |5589.02986907959 |\n",
      "|TOSHIBA HDWE160     |5589.02986907959      |5589.02986907959 |\n",
      "|HGST HMS5C4040BLE640|-9.313225746154785E-10|3726.023277282715|\n",
      "|ST4000DM005         |-9.313225746154785E-10|3726.023277282715|\n",
      "+--------------------+----------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "backblaze_2019_q3.groupBy('model') \\\n",
    "    .agg(\n",
    "        F.min(F.col('capacity_bytes') / F.pow(F.lit(1024), 3)).alias('min_GB'), \n",
    "        F.max(F.col('capacity_bytes') / F.pow(F.lit(1024), 3)).alias('max_GB')\n",
    "    ).select('model', 'min_GB', 'max_GB') \\\n",
    "    .orderBy('max_GB', ascending=False) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:====================================================>   (29 + 2) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+----------------------+------------------+\n",
      "|model                              |min_GB                |max_GB            |\n",
      "+-----------------------------------+----------------------+------------------+\n",
      "|Seagate BarraCuda SSD ZA250CM10002 |232.88591766357422    |232.88591766357422|\n",
      "|Seagate SSD                        |232.88591766357422    |232.88591766357422|\n",
      "|ST9250315AS                        |232.88591766357422    |232.88591766357422|\n",
      "|ST320LT007                         |298.09114837646484    |298.09114837646484|\n",
      "|DELLBOSS VD                        |447.06915283203125    |447.06915283203125|\n",
      "|WDC WD5000LPVX                     |465.7617416381836     |465.7617416381836 |\n",
      "|WDC WD5000BPKT                     |465.7617416381836     |465.7617416381836 |\n",
      "|WDC WD5000LPCX                     |465.7617416381836     |465.7617416381836 |\n",
      "|TOSHIBA MQ01ABF050                 |465.7617416381836     |465.7617416381836 |\n",
      "|ST500LM021                         |465.7617416381836     |465.7617416381836 |\n",
      "|ST500LM030                         |465.7617416381836     |465.7617416381836 |\n",
      "|ST500LM012 HN                      |465.7617416381836     |465.7617416381836 |\n",
      "|Seagate BarraCuda SSD ZA500CM10002 |465.7617416381836     |465.7617416381836 |\n",
      "|TOSHIBA MQ01ABF050M                |465.7617416381836     |465.7617416381836 |\n",
      "|ST1000LM024 HN                     |931.5133895874023     |931.5133895874023 |\n",
      "|Seagate BarraCuda SSD ZA2000CM10002|1863.0166854858398    |1863.0166854858398|\n",
      "|HGST HMS5C4040BLE640               |-9.313225746154785E-10|3726.023277282715 |\n",
      "|TOSHIBA MD04ABA400V                |3726.023277282715     |3726.023277282715 |\n",
      "|ST4000DM000                        |-9.313225746154785E-10|3726.023277282715 |\n",
      "|HGST HDS5C4040ALE630               |3726.023277282715     |3726.023277282715 |\n",
      "+-----------------------------------+----------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT\n",
    "        model,\n",
    "        MIN(capacity_bytes / POW(1024, 3)) AS min_GB,\n",
    "        MAX(capacity_bytes / POW(1024, 3)) AS max_GB\n",
    "    FROM backblaze_2019_q3\n",
    "    GROUP BY model\n",
    "    ORDER BY max_GB ASC\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the number of days of operation that a model has and the number of HDD failures and save to 2 different SQL views."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PySpark version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 111:=====================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|model               |drive_days|\n",
      "+--------------------+----------+\n",
      "|ST12000NM0007       |3212635   |\n",
      "|ST4000DM000         |1796728   |\n",
      "|ST8000NM0055        |1324122   |\n",
      "|HGST HMS5C4040BLE640|1173136   |\n",
      "|HGST HUH721212ALN604|946724    |\n",
      "|ST8000DM002         |906588    |\n",
      "|HGST HMS5C4040ALE640|245904    |\n",
      "|HGST HUH721212ALE600|122200    |\n",
      "|TOSHIBA MG07ACA14TA |112235    |\n",
      "|ST10000NM0086       |110282    |\n",
      "|HGST HUH728080ALE600|92092     |\n",
      "|ST6000DX000         |81512     |\n",
      "|ST500LM012 HN       |46309     |\n",
      "|TOSHIBA MQ01ABF050  |44808     |\n",
      "|TOSHIBA MQ01ABF050M |35351     |\n",
      "|ST500LM030          |21447     |\n",
      "|WDC WD5000LPVX      |19723     |\n",
      "|TOSHIBA MD04ABA400V |9108      |\n",
      "|WDC WD5000LPCX      |4967      |\n",
      "|Seagate SSD         |4734      |\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "drive_days = backblaze_2019_q3.groupBy('model') \\\n",
    "    .agg(F.count('*').alias('drive_days')) \\\n",
    "    .select('model', 'drive_days') \\\n",
    "    .orderBy('drive_days', ascending=False)\n",
    "\n",
    "drive_days.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 162:=====================================================> (30 + 1) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------------+\n",
      "|model                             |number_of_failures|\n",
      "+----------------------------------+------------------+\n",
      "|ST12000NM0007                     |364               |\n",
      "|ST4000DM000                       |72                |\n",
      "|ST8000NM0055                      |50                |\n",
      "|ST8000DM002                       |36                |\n",
      "|TOSHIBA MQ01ABF050                |25                |\n",
      "|HGST HMS5C4040BLE640              |19                |\n",
      "|HGST HUH721212ALN604              |15                |\n",
      "|ST500LM030                        |9                 |\n",
      "|ST500LM012 HN                     |7                 |\n",
      "|HGST HMS5C4040ALE640              |6                 |\n",
      "|ST12000NM0117                     |5                 |\n",
      "|TOSHIBA MQ01ABF050M               |5                 |\n",
      "|ST6000DX000                       |4                 |\n",
      "|TOSHIBA MG07ACA14TA               |2                 |\n",
      "|HGST HUH721212ALE600              |2                 |\n",
      "|ST10000NM0086                     |2                 |\n",
      "|ST8000DM005                       |1                 |\n",
      "|HGST HUH728080ALE600              |1                 |\n",
      "|Seagate BarraCuda SSD ZA500CM10002|1                 |\n",
      "|WDC WD5000LPVX                    |1                 |\n",
      "+----------------------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "failures = backblaze_2019_q3.where(F.col('failure') == 1) \\\n",
    "    .groupBy('model') \\\n",
    "    .agg(F.count('*').alias('number_of_failures')) \\\n",
    "    .select('model', 'number_of_failures') \\\n",
    "    .orderBy('number_of_failures', ascending=False)\n",
    "\n",
    "failures.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='backblaze_2019_q3', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='drive_days', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='elements', database=None, description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='failures', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive_days.createOrReplaceTempView('drive_days')\n",
    "failures.createOrReplaceTempView('failures')\n",
    "\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SQL version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 136:===================================================>   (29 + 2) / 31]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|model               |drive_days|\n",
      "+--------------------+----------+\n",
      "|ST12000NM0007       |3212635   |\n",
      "|ST4000DM000         |1796728   |\n",
      "|ST8000NM0055        |1324122   |\n",
      "|HGST HMS5C4040BLE640|1173136   |\n",
      "|HGST HUH721212ALN604|946724    |\n",
      "|ST8000DM002         |906588    |\n",
      "|HGST HMS5C4040ALE640|245904    |\n",
      "|HGST HUH721212ALE600|122200    |\n",
      "|TOSHIBA MG07ACA14TA |112235    |\n",
      "|ST10000NM0086       |110282    |\n",
      "|HGST HUH728080ALE600|92092     |\n",
      "|ST6000DX000         |81512     |\n",
      "|ST500LM012 HN       |46309     |\n",
      "|TOSHIBA MQ01ABF050  |44808     |\n",
      "|TOSHIBA MQ01ABF050M |35351     |\n",
      "|ST500LM030          |21447     |\n",
      "|WDC WD5000LPVX      |19723     |\n",
      "|TOSHIBA MD04ABA400V |9108      |\n",
      "|WDC WD5000LPCX      |4967      |\n",
      "|Seagate SSD         |4734      |\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT model, COUNT(*) drive_days\n",
    "    FROM backblaze_2019_q3\n",
    "    GROUP BY model\n",
    "    ORDER BY drive_days DESC\n",
    "    \"\"\"\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|         |backblaze_2019_q3|       true|\n",
      "|         |       drive_days|       true|\n",
      "|         |         elements|       true|\n",
      "+---------+-----------------+-----------+\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "++\n",
      "||\n",
      "++\n",
      "++\n",
      "\n",
      "+---------+-----------------+-----------+\n",
      "|namespace|        tableName|isTemporary|\n",
      "+---------+-----------------+-----------+\n",
      "|         |backblaze_2019_q3|       true|\n",
      "|         |       drive_days|       true|\n",
      "|         |         elements|       true|\n",
      "+---------+-----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW TABLES\"\"\").show()\n",
    "\n",
    "spark.sql(\"\"\"DROP VIEW IF EXISTS drive_days\"\"\").show()\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW drive_days AS\n",
    "        SELECT model, COUNT(*) drive_days\n",
    "        FROM backblaze_2019_q3\n",
    "        GROUP BY model\n",
    "        ORDER BY drive_days DESC\n",
    "    \"\"\"\n",
    ").show(truncate=False)\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"SHOW TABLES\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d76f782f9e2bb336f39952c692d78dbf55a20fd2f5486713d2fa651d7c15ab40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
